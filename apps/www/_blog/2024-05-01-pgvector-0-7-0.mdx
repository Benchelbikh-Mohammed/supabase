---
title: "What's new in pgvector v0.7.0"
description: 'Exploring new features in pgvector v0.7.0'
author: pavel
image: pgvector-0-7-0.png
thumb: pgvector-0-7-0.png
categories:
  - engineering
tags:
  - supabase-engineering
  - planetpg
date: '2024-05-01'
toc_depth: 3
---

### New vector types

Real-world embedding datasets contain a lot of redundancy. It’s buried in the vector space. For example, when vectors are clustered around some central points in multidimensional, we’re seeing exploitable structure. Reducing this makes it possible to get memory and performance savings with no effect on precision. There are several different approaches to employ this ideas made available in pgvector since 0.7.0:

- float16 vector representation
- sparse vectors
- bit vectors

### Float16 vectors

An HNSW index is are most efficient when the index fits into shared memory and is not evicted due to concurrent Postgres operations. The reason for this is that it allows Postgres to minimize costly IO. Historically, pgvector supported only 32-bit vectors. In version 0.7.0 pgvector introduces 16-bit float HNSW indexes which consume exactly half the memory. That reduction in memory keeps operations at maximum performance for twice as long.

There are two options when using float16 vectors:

- Index using float16, but the underlying table continues to use float32
- The index and the underlying table both use float16. This options uses 50% as much disk space in addition to requiring 50% less shared memory to operate efficiently. Performance is further improved with more vectors fitting in a single Postgres page and with fewer page evictions due to concurrent operations..

To duplicate existing float32 embedding table to float16 one:

```sql
create table embedding_half (
    id serial,
    vector halfvec(1536),
    primary key (id)
);

insert into embedding_half (vector)
select
	vector::halfvec(1536)
from
	embedding_full;
```

With 900K OpenAI 1536-dimensional vectors the table size is 3.5Gb. For comparison, embedding_full required 7Gb.

Then we can build a float16 HNSW index :

```sql
CREATE INDEX ON embedding_half USING hnsw (vector halfvec_l2_ops);
```

For testing index creation performance we chose a c7g.metal instance with 128Gb memory and parameters:

```
shared_buffers = 50000MB
maintenance_work_mem = 30000MB
max_parallel_maintenance_workers = {0-63}
wal_level=minimal
max_wal_size = 10GB
autovacuum = off
full_page_writes = off
fsync = off
```

<Img
  alt="security-peformance-advisor"
  src="/images/blog/pgvector-0-7-0-perf.png"
  captionAlign="left"
  zoomable={false}
/>

HNSW build times recently experienced a stepwise improvement in the 0.6.2 release, which introduced parallel builds. 0.7.0 with the halfvec (float16) feature improves that speedup a further 30%.

Note that float16 vector arithmetic on ARM architecture is identical to float32, so that serial build time (with one parallel worker) has not improved. Still there is a significant difference for parallel build due to better pages/IO utilization. I haven’t used pre-warming and other artificial things in this test. Both heap and HNSW relations for float16 occupy only half of the space compared to the previous float32 ones.

There is a proposal to speed it up even more in the future by using SVE intrinsic on ARM architecture (link: https://github.com/pgvector/pgvector/pull/536).

Jonathan Katz [made his measurements](https://jkatz05.com/post/postgres/pgvector-performance-150x-speedup) on HNSW performance on r7gd.16xlarge (64 vCPU, 512GiB RAM), and his results are even better. For float16, HNSW build time is up to 3x faster. For select’s performance, ANN benchmark results show that precision is not changed with decreasing bitness, and queries per second (QPS) is similar for in-memory cases. But when in real machine queries are using IO or there is some HNSW pages eviction from memory due to concurrent connections, there would be a meaningful difference. With only half of memory needed to accommodate the same HNSW index, cost for the same performance and precision is also significantly less.

|                            | Vector / Vector | Vector / HalfVec |
| -------------------------- | --------------- | ---------------- |
| **Index size (MB)**        | 7734            | 3867             |
| **Index build time (s)**   | 264             | 90               |
| **Recall @ ef_search=10**  | 0.819           | 0.809            |
| **QPS @ ef_search=10**     | 1231            | 1219             |
| **Recall @ ef_search=40**  | 0.945           | 0.945            |
| **QPS @ ef_search=40**     | 627             | 642              |
| **Recall @ ef_search=200** | 0.987           | 0.987            |
| **QPS @ ef_search=200**    | 191             | 190              |

Full results on different datasets see [this GitHub issue](https://github.com/pgvector/pgvector/issues/326#issuecomment-2028467121).

### Sparse vectors

If vectors contain many zero components, then a sparse vector representation can save significant storage space. For example, to populate sparse vectors:

```sql
create embedding_sparse (
    id serial,
    vector sparsevec(1536),
    primary key (id)
)

INSERT INTO embedding_sparse (embedding) VALUES ('{1:0.1,3:0.2,5:0.3}/1536'), ('{1:0.4,3:0.5,5:0.6}/1536');
```

The sparse vector only consumes storage space required non-zero components. In this case, thats 3 values in a 1536 vector.

Note the new vector syntax `{1:3,3:1,5:2}/1536` for the sparse vector representation in:

```sql
SELECT * FROM embedding_sparse ORDER BY vector <-> '{1:3,3:1,5:2}/1536' LIMIT 5;
```

### Bit vectors

Using binary quantization we can represent float vector as a vector in binary space. This reduces storage size dramatically and is intended as a way to quickly “pre-select” from a data set before performaing an additional search within the subset. When properly parameterized, the secondary select can be very fast, even without index.

```sql
CREATE INDEX ON embedding
	USING hnsw ((binary_quantize(vector)::bit(1000)) bit_hamming_ops);

SELECT
	*
FROM
	embedding
ORDER BY
	binary_quantize(vector)::bit(3) <~> binary_quantize('[1,-2,3]')
LIMIT 5;
```

To use binary quantized HNSW index to pre-select from a larger dataset and then make a fast selection from the resulting subset, without an index.

```sql
SELECT * FROM (
    SELECT
	    *
    FROM
	    embedding
	  ORDER BY
		  binary_quantize(vector)::bit(3) <~> binary_quantize('[1,-2,3]')
		LIMIT 20
)
ORDER BY
	vector <=> '[1,-2,3]'
LIMIT 5;
```

It allows build a small and fast HNSW index for select, insert or update operations, and still have fast vector search. Exact configuration the `limit` clauses is data dependent, so you’ll want to experiment with the sub-select size and number of final results directly on your own dataset.

### New distance functions

Pgvector 0.7.0 also added support for L1 distance operator `<+>`

New distance types for indexing:

[L1 distance](https://en.wikipedia.org/wiki/Taxicab_geometry) - added in 0.7.0

```sql
CREATE INDEX ON items USING hnsw (embedding vector_l1_ops);
```

[Hamming distance](https://www.notion.so/What-s-new-in-pgvector-0-7-0-fd27d50c8d7e4186a0a4a4bc8b69c7ab?pvs=21) - added in 0.7.0

```sql
CREATE INDEX ON items USING hnsw (embedding bit_hamming_ops);
```

[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) - added in 0.7.0

```sql
CREATE INDEX ON vector USING hnsw (vector bit_jaccard_ops);
```

### Conclusion

Over the last year pgvector had significant development both is functionality and performance. HNSW, parallel build, and many other options were introduced. In many cases now we have +100 times speedup compared to one year ago.

For a more complete comparison of pgvector performance over the last year check out [this post](https://jkatz05.com/post/postgres/pgvector-performance-150x-speedup/).
